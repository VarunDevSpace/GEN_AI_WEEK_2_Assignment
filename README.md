# ğŸš€ GenAI_Assignment_2

ğŸ§  **Python implementation of common neural network activation functions with formulas and sample inputs.**

---

## ğŸ“Œ Overview
This repository contains a simple and clear Python implementation of **popular activation functions** used in **Artificial Neural Networks and Deep Learning**.  
Each activation function includes:
- ğŸ“ Mathematical formula (as comments)
- ğŸ§ª Sample input values
- âš™ï¸ NumPy-based implementation

---

## ğŸ”¢ Activation Functions Implemented
âœ” Sigmoid   
âœ” Tanh   
âœ” ReLU   
âœ” Leaky ReLU   
âœ” Softmax  
âœ” Swish   
âœ” Maxout    

---

## ğŸ›  Technologies Used
- ğŸ Python  
- ğŸ“¦ NumPy  

---
